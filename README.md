
## Neural Machine Translation

This project implements the Transformer architecture **from scratch in PyTorch** for a translation task. All core components—**multi-head self-attention**, **positional encoding**, and the **encoder-decoder structure**—are custom-built without relying on high-level implementations.

The model is trained on the **Bilingual OPUS Books dataset** from Hugging Face, enabling multilingual translation using parallel text corpora. The project demonstrates how to construct and train a fully functional neural machine translation pipeline starting from raw data processing to generating translated text.

